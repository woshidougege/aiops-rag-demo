"""
AIOps RAG Demo - LangChain æ¡†æ¶ç‰ˆæœ¬
"""

from fastapi import FastAPI, HTTPException
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
from typing import Optional, List, Dict
import uvicorn
import os
import json

# LangChain æ ¸å¿ƒç»„ä»¶
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.vectorstores import Milvus
from langchain_core.documents import Document

from config import LLM_CONFIG, EMBEDDING_CONFIG, MILVUS_CONFIG


# ===================================================================
# åˆå§‹åŒ–
# ===================================================================
app = FastAPI(
    title="AIOps RAG Demo",
    description="æ™ºèƒ½è¿ç»´æ•…éšœè¯Šæ–­ç³»ç»Ÿ",
    version="1.0.0"
)

print("ğŸš€ å¯åŠ¨ AIOps RAG Demo...")


# ===================================================================
# RAGå¼•æ“ï¼ˆLangChain ç‰ˆï¼‰
# ===================================================================
class LangChainRAGEngine:
    """åŸºäº LangChain çš„ RAG å¼•æ“"""
    
    def __init__(self):
        print("ğŸš€ åˆå§‹åŒ– LangChain RAG å¼•æ“...")
        
        # åˆå§‹åŒ– LLM
        self.llm = ChatOpenAI(
            base_url=LLM_CONFIG['api_base'],
            api_key=LLM_CONFIG['api_key'],
            model=LLM_CONFIG['model'],
            temperature=LLM_CONFIG['temperature'],
            max_tokens=LLM_CONFIG['max_tokens']
        )
        print(f"âœ“ LLM å·²åˆå§‹åŒ–: {LLM_CONFIG['model']}")
        
        # åˆå§‹åŒ– Embeddings
        self.embeddings = OpenAIEmbeddings(
            base_url=EMBEDDING_CONFIG['api_url'].replace('/embeddings', ''),
            api_key=EMBEDDING_CONFIG['api_key'],
            model=EMBEDDING_CONFIG['model']
        )
        print(f"âœ“ Embedding å·²åˆå§‹åŒ–: {EMBEDDING_CONFIG['model']}")
        
        # åŠ è½½çŸ¥è¯†åº“å¹¶åˆå§‹åŒ–å‘é‡å­˜å‚¨
        self.vectorstore = None
        self.knowledge_base = []
        self.load_knowledge()
        
        # åˆ›å»ºè¯Šæ–­æç¤ºæ¨¡æ¿
        self.diagnosis_prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ AIOps è¿ç»´ä¸“å®¶ï¼Œæ“…é•¿åˆ†æç³»ç»Ÿæ•…éšœå¹¶æä¾›è§£å†³æ–¹æ¡ˆã€‚"),
            ("user", """è¯·åˆ†æä»¥ä¸‹æ•…éšœå¹¶ä»¥JSONæ ¼å¼è¾“å‡ºè¯Šæ–­ç»“æœã€‚

ã€å½“å‰æ•…éšœã€‘
{error_log}

ã€å†å²ç›¸ä¼¼æ¡ˆä¾‹ã€‘
{retrieved_cases}

ã€è¾“å‡ºè¦æ±‚ã€‘
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼ˆä¸è¦æœ‰å…¶ä»–æ–‡å­—ï¼‰ï¼š
{{
  "diagnosis": "æ•…éšœè¯Šæ–­ï¼ˆä¸€å¥è¯æ¦‚æ‹¬ï¼‰",
  "root_cause": "æ ¹æœ¬åŸå› åˆ†æï¼ˆæ·±å…¥æŠ€æœ¯ç»†èŠ‚ï¼‰",
  "solution": "è§£å†³æ–¹æ¡ˆï¼ˆåˆ†æ­¥éª¤ï¼Œå¯æ‰§è¡Œï¼‰",
  "confidence": 0.85
}}""")
        ])
        
        # åˆ›å»ºè¯Šæ–­é“¾
        self.diagnosis_chain = (
            self.diagnosis_prompt 
            | self.llm 
            | StrOutputParser()
        )
        
        print("âœ“ RAG å¼•æ“åˆå§‹åŒ–å®Œæˆ\n")
    
    def load_knowledge(self):
        """åŠ è½½çŸ¥è¯†åº“åˆ°å‘é‡å­˜å‚¨"""
        kb_path = "data/knowledge_base.json"
        if os.path.exists(kb_path):
            with open(kb_path, 'r', encoding='utf-8') as f:
                self.knowledge_base = json.load(f)
            print(f"âœ“ åŠ è½½äº† {len(self.knowledge_base)} æ¡çŸ¥è¯†æ¡ˆä¾‹")
            
            # è½¬æ¢ä¸º LangChain Documents
            documents = []
            for case in self.knowledge_base:
                content = f"""é”™è¯¯ç±»å‹: {case['error_type']}
æ—¥å¿—å†…å®¹: {case.get('log_content', '')}
æ ¹æœ¬åŸå› : {case['root_cause']}
è§£å†³æ–¹æ¡ˆ: {case['solution']}"""
                
                metadata = {
                    "error_type": case['error_type'],
                    "root_cause": case['root_cause'],
                    "solution": case['solution'],
                    "severity": case.get('severity', 'medium')
                }
                documents.append(Document(page_content=content, metadata=metadata))
            
            # ä½¿ç”¨å†…å­˜å‘é‡å­˜å‚¨ï¼ˆå¦‚éœ€æŒä¹…åŒ–å¯æ”¹ç”¨ Milvusï¼‰
            from langchain_community.vectorstores import FAISS
            try:
                self.vectorstore = FAISS.from_documents(documents, self.embeddings)
                print("âœ“ å‘é‡å­˜å‚¨å·²åˆ›å»ºï¼ˆä½¿ç”¨ FAISSï¼‰")
            except Exception as e:
                print(f"âš  å‘é‡å­˜å‚¨åˆ›å»ºå¤±è´¥: {e}")
                self.vectorstore = None
        else:
            print(f"âš  çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {kb_path}")
    
    def search(self, query: str, top_k: int = 3) -> List[Dict]:
        """æ£€ç´¢ç›¸ä¼¼æ¡ˆä¾‹"""
        print(f"ğŸ” æ£€ç´¢: {query[:50]}...")
        
        if not self.vectorstore:
            print("âš  å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨ç®€å•åŒ¹é…")
            return self.simple_search(query, top_k)
        
        try:
            # ä½¿ç”¨ LangChain çš„ç›¸ä¼¼åº¦æœç´¢
            docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=top_k)
            
            results = []
            for doc, score in docs_with_scores:
                # è½¬æ¢åˆ†æ•°ä¸ºç›¸ä¼¼åº¦ (FAISS è¿”å›çš„æ˜¯è·ç¦»ï¼Œè¶Šå°è¶Šç›¸ä¼¼)
                similarity = 1.0 / (1.0 + score)
                results.append({
                    "error_type": doc.metadata.get('error_type', 'æœªçŸ¥'),
                    "root_cause": doc.metadata.get('root_cause', ''),
                    "solution": doc.metadata.get('solution', ''),
                    "severity": doc.metadata.get('severity', 'medium'),
                    "similarity": round(similarity, 3),
                    "content": doc.page_content
                })
            
            print(f"âœ“ æ‰¾åˆ° {len(results)} ä¸ªç›¸ä¼¼æ¡ˆä¾‹")
            return results
            
        except Exception as e:
            print(f"âš  å‘é‡æ£€ç´¢å¤±è´¥: {e}")
            return self.simple_search(query, top_k)
    
    def simple_search(self, query: str, top_k: int = 3) -> List[Dict]:
        """ç®€å•çš„å…³é”®è¯åŒ¹é…æœç´¢ï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        results = []
        query_lower = query.lower()
        
        for case in self.knowledge_base:
            case_text = f"{case['error_type']} {case.get('log_content', '')} {case['root_cause']}".lower()
            # ç®€å•çš„å…³é”®è¯åŒ¹é…
            common_words = set(query_lower.split()) & set(case_text.split())
            similarity = len(common_words) / max(len(query_lower.split()), 1)
            
            if similarity > 0:
                results.append({
                    **case,
                    "similarity": round(similarity, 3)
                })
        
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:top_k]
    
    def diagnose(self, error_log: str) -> Dict:
        """å®Œæ•´è¯Šæ–­æµç¨‹"""
        print("ğŸ”® å¼€å§‹è¯Šæ–­...")
        
        # 1. RAG æ£€ç´¢ç›¸ä¼¼æ¡ˆä¾‹
        cases = self.search(error_log, top_k=3)
        
        # 2. æ ¼å¼åŒ–æ£€ç´¢ç»“æœ
        if cases:
            history_text = ""
            for i, case in enumerate(cases, 1):
                history_text += f"\næ¡ˆä¾‹{i}ï¼ˆç›¸ä¼¼åº¦: {case.get('similarity', 0):.2f}ï¼‰:\n"
                history_text += f"  é”™è¯¯ç±»å‹: {case['error_type']}\n"
                history_text += f"  æ ¹æœ¬åŸå› : {case['root_cause']}\n"
                history_text += f"  è§£å†³æ–¹æ¡ˆ: {case['solution']}\n"
        else:
            history_text = "æ— ç›¸ä¼¼å†å²æ¡ˆä¾‹"
        
        # 3. ä½¿ç”¨ LangChain é“¾è¿›è¡Œè¯Šæ–­
        try:
            print("ğŸ¤– è°ƒç”¨ LLM ç”Ÿæˆè¯Šæ–­...")
            response = self.diagnosis_chain.invoke({
                "error_log": error_log,
                "retrieved_cases": history_text
            })
            
            # 4. è§£æ JSON ç»“æœ
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
            else:
                result = json.loads(response)
            
            result['retrieved_cases'] = cases
            print("âœ“ è¯Šæ–­å®Œæˆ")
            return result
            
        except Exception as e:
            print(f"âš  LLM è°ƒç”¨å¤±è´¥: {e}")
            # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªç›¸ä¼¼æ¡ˆä¾‹
            if cases:
                return {
                    "diagnosis": f"åŸºäºå†å²æ¡ˆä¾‹çš„è¯Šæ–­ï¼š{cases[0]['error_type']}",
                    "root_cause": cases[0]['root_cause'],
                    "solution": cases[0]['solution'],
                    "confidence": cases[0].get('similarity', 0.5),
                    "retrieved_cases": cases
                }
            else:
                return {
                    "diagnosis": "æ— æ³•è‡ªåŠ¨è¯Šæ–­",
                    "root_cause": "æœªæ‰¾åˆ°ç›¸ä¼¼æ¡ˆä¾‹ï¼Œå»ºè®®äººå·¥æ’æŸ¥",
                    "solution": "è¯·æä¾›æ›´å¤šæ—¥å¿—ä¿¡æ¯æˆ–è”ç³»è¿ç»´å›¢é˜Ÿ",
                    "confidence": 0.0,
                    "retrieved_cases": []
                }


# åˆå§‹åŒ–RAGå¼•æ“
rag_engine = LangChainRAGEngine()


# ===================================================================
# æ•°æ®æ¨¡å‹
# ===================================================================
class DiagnosisRequest(BaseModel):
    error_log: str
    top_k: Optional[int] = 3


class DiagnosisResponse(BaseModel):
    success: bool
    diagnosis: str
    root_cause: str
    solution: str
    confidence: float
    retrieved_cases: list


# ===================================================================
# APIè·¯ç”±
# ===================================================================
@app.get("/", response_class=HTMLResponse)
async def index():
    """ä¸»é¡µ"""
    html_path = "templates/index.html"
    if os.path.exists(html_path):
        with open(html_path, 'r', encoding='utf-8') as f:
            return HTMLResponse(content=f.read())
    return HTMLResponse("<h1>AIOps RAG Demo</h1><p><a href='/docs'>APIæ–‡æ¡£</a></p>")


@app.get("/health")
async def health():
    return {"status": "healthy", "knowledge_base": len(rag_engine.knowledge_base)}


@app.post("/api/diagnose", response_model=DiagnosisResponse)
async def diagnose(request: DiagnosisRequest):
    """è¯Šæ–­API"""
    try:
        result = rag_engine.diagnose(request.error_log)
        return DiagnosisResponse(
            success=True,
            diagnosis=result.get('diagnosis', ''),
            root_cause=result.get('root_cause', ''),
            solution=result.get('solution', ''),
            confidence=float(result.get('confidence', 0.5)),
            retrieved_cases=[
                {
                    "error_type": c['error_type'],
                    "similarity": c.get('similarity', 0),
                    "root_cause": c['root_cause'],
                    "solution": c['solution']
                }
                for c in result.get('retrieved_cases', [])
            ]
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ===================================================================
# å¯åŠ¨
# ===================================================================
if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸŒ è®¿é—®åœ°å€: http://localhost:8888")
    print("ğŸ“š APIæ–‡æ¡£: http://localhost:8888/docs")
    print("="*60 + "\n")
    
    uvicorn.run(app, host="0.0.0.0", port=8888, log_level="info")
