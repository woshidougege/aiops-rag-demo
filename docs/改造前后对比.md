# AIOps RAG Demo - æ”¹é€ å‰åå¯¹æ¯”

## ğŸ“Š æ ¸å¿ƒå˜åŒ–æ€»è§ˆ

| ç»´åº¦ | æ”¹é€ å‰ | æ”¹é€ å (LangChain) |
|------|--------|-------------------|
| **æ¡†æ¶** | æ‰‹åŠ¨å®ç° | LangChain æ ‡å‡†æ¡†æ¶ |
| **ä»£ç å¤æ‚åº¦** | é«˜ï¼ˆæ‰‹åŠ¨ç®¡ç†ï¼‰ | ä½ï¼ˆå£°æ˜å¼ï¼‰ |
| **å¯æ‰©å±•æ€§** | ä¸­ | é«˜ |
| **ç»´æŠ¤æˆæœ¬** | é«˜ | ä½ |
| **ç¤¾åŒºæ”¯æŒ** | æ—  | æ´»è·ƒ |
| **å‘é‡å­˜å‚¨** | æ‰‹åŠ¨å®ç°ç›¸ä¼¼åº¦è®¡ç®— | FAISS/Milvus å°è£… |
| **æç¤ºè¯ç®¡ç†** | å­—ç¬¦ä¸²æ‹¼æ¥ | ChatPromptTemplate |
| **LLM è°ƒç”¨** | requests æ‰‹åŠ¨è¯·æ±‚ | ChatOpenAI ç»Ÿä¸€æ¥å£ |
| **é”™è¯¯å¤„ç†** | æ‰‹åŠ¨ try-except | å†…ç½®é™çº§æœºåˆ¶ |

---

## ğŸ”„ ä»£ç å¯¹æ¯”

### 1. LLM è°ƒç”¨

#### æ”¹é€ å‰
```python
def call_llm(self, prompt: str) -> str:
    """è°ƒç”¨LLM"""
    response = requests.post(
        f"{LLM_CONFIG['api_base']}/chat/completions",
        headers={"Authorization": f"Bearer {LLM_CONFIG['api_key']}"},
        json={
            "model": LLM_CONFIG['model'],
            "messages": [{"role": "user", "content": prompt}],
            "temperature": LLM_CONFIG['temperature'],
            "max_tokens": LLM_CONFIG['max_tokens']
        },
        timeout=60
    )
    if response.status_code == 200:
        return response.json()['choices'][0]['message']['content']
    return ""
```

#### æ”¹é€ å
```python
# åˆå§‹åŒ–
self.llm = ChatOpenAI(
    base_url=LLM_CONFIG['api_base'],
    api_key=LLM_CONFIG['api_key'],
    model=LLM_CONFIG['model'],
    temperature=LLM_CONFIG['temperature'],
    max_tokens=LLM_CONFIG['max_tokens']
)

# è°ƒç”¨
response = self.llm.invoke("your prompt")
```

**ä¼˜åŠ¿**ï¼š
- âœ… ç»Ÿä¸€æ¥å£ï¼Œæ˜“äºåˆ‡æ¢æ¨¡å‹
- âœ… è‡ªåŠ¨é‡è¯•å’Œé”™è¯¯å¤„ç†
- âœ… æ”¯æŒæµå¼è¾“å‡º
- âœ… å†…ç½® token è®¡æ•°

---

### 2. Embeddings å‘é‡åŒ–

#### æ”¹é€ å‰
```python
def get_embedding(self, text: str) -> List[float]:
    """è·å–æ–‡æœ¬å‘é‡"""
    response = requests.post(
        EMBEDDING_CONFIG['api_url'],
        headers={"Authorization": f"Bearer {EMBEDDING_CONFIG['api_key']}"},
        json={"model": EMBEDDING_CONFIG['model'], "input": text},
        timeout=10
    )
    if response.status_code == 200:
        return response.json()['data'][0]['embedding']
    return []
```

#### æ”¹é€ å
```python
# åˆå§‹åŒ–
self.embeddings = OpenAIEmbeddings(
    base_url=EMBEDDING_CONFIG['api_url'].replace('/embeddings', ''),
    api_key=EMBEDDING_CONFIG['api_key'],
    model=EMBEDDING_CONFIG['model']
)

# è°ƒç”¨
vector = self.embeddings.embed_query(text)
```

**ä¼˜åŠ¿**ï¼š
- âœ… æ ‡å‡†åŒ–æ¥å£
- âœ… æ‰¹é‡å‘é‡åŒ–ä¼˜åŒ–
- âœ… è‡ªåŠ¨ç¼“å­˜
- âœ… å¼‚å¸¸å¤„ç†

---

### 3. å‘é‡å­˜å‚¨å’Œæ£€ç´¢

#### æ”¹é€ å‰
```python
def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    import math
    dot_product = sum(a * b for a, b in zip(vec1, vec2))
    norm1 = math.sqrt(sum(a * a for a in vec1))
    norm2 = math.sqrt(sum(b * b for b in vec2))
    if norm1 == 0 or norm2 == 0:
        return 0.0
    return dot_product / (norm1 * norm2)

def search(self, query: str, top_k: int = 3) -> List[Dict]:
    """æ£€ç´¢ç›¸ä¼¼æ¡ˆä¾‹"""
    query_vec = self.get_embedding(query)
    results = []
    for case in self.knowledge_base:
        case_text = f"{case['error_type']} {case['log_content']} {case['root_cause']}"
        case_vec = self.get_embedding(case_text)
        if case_vec:
            similarity = self.cosine_similarity(query_vec, case_vec)
            results.append({**case, "similarity": similarity})
    
    results.sort(key=lambda x: x['similarity'], reverse=True)
    return results[:top_k]
```

#### æ”¹é€ å
```python
# åˆå§‹åŒ–å‘é‡å­˜å‚¨
from langchain_community.vectorstores import FAISS

documents = [Document(page_content=..., metadata=...) for case in cases]
self.vectorstore = FAISS.from_documents(documents, self.embeddings)

# æ£€ç´¢
docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=top_k)
```

**ä¼˜åŠ¿**ï¼š
- âœ… ä½¿ç”¨ä¼˜åŒ–çš„å‘é‡æ£€ç´¢ç®—æ³•ï¼ˆFAISSï¼‰
- âœ… æ”¯æŒå¤šç§ç›¸ä¼¼åº¦åº¦é‡
- âœ… å¯æŒä¹…åŒ–å­˜å‚¨
- âœ… æ— éœ€æ‰‹åŠ¨è®¡ç®—ç›¸ä¼¼åº¦
- âœ… æ€§èƒ½æ›´é«˜ï¼ˆC++ åº•å±‚å®ç°ï¼‰

---

### 4. æç¤ºè¯ç®¡ç†

#### æ”¹é€ å‰
```python
prompt = f"""ä½ æ˜¯AIOpsä¸“å®¶ã€‚åˆ†æä»¥ä¸‹æ•…éšœå¹¶ä»¥JSONæ ¼å¼è¾“å‡ºè¯Šæ–­ã€‚

å½“å‰æ•…éšœï¼š
{error_log}

å†å²æ¡ˆä¾‹ï¼š
{history_text if history_text else 'æ— '}

è¾“å‡ºJSONæ ¼å¼ï¼š
{{
  "diagnosis": "æ•…éšœè¯Šæ–­",
  "root_cause": "æ ¹æœ¬åŸå› ",
  "solution": "è§£å†³æ–¹æ¡ˆ",
  "confidence": 0.85
}}"""
```

#### æ”¹é€ å
```python
from langchain_core.prompts import ChatPromptTemplate

self.diagnosis_prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ AIOps è¿ç»´ä¸“å®¶ï¼Œæ“…é•¿åˆ†æç³»ç»Ÿæ•…éšœå¹¶æä¾›è§£å†³æ–¹æ¡ˆã€‚"),
    ("user", """è¯·åˆ†æä»¥ä¸‹æ•…éšœå¹¶ä»¥JSONæ ¼å¼è¾“å‡ºè¯Šæ–­ç»“æœã€‚

ã€å½“å‰æ•…éšœã€‘
{error_log}

ã€å†å²ç›¸ä¼¼æ¡ˆä¾‹ã€‘
{retrieved_cases}

ã€è¾“å‡ºè¦æ±‚ã€‘
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
{{
  "diagnosis": "...",
  "root_cause": "...",
  "solution": "...",
  "confidence": 0.85
}}""")
])
```

**ä¼˜åŠ¿**ï¼š
- âœ… ç»“æ„åŒ–æç¤ºè¯ç®¡ç†
- âœ… æ”¯æŒæ¨¡æ¿å¤ç”¨
- âœ… è§’è‰²æ˜ç¡®ï¼ˆsystem/userï¼‰
- âœ… æ˜“äºç‰ˆæœ¬æ§åˆ¶

---

### 5. RAG Chain ç»„åˆ

#### æ”¹é€ å‰
```python
def diagnose(self, error_log: str) -> Dict:
    """å®Œæ•´è¯Šæ–­"""
    # 1. æ£€ç´¢
    cases = self.search(error_log)
    
    # 2. æ„å»ºæç¤ºè¯
    history_text = build_history(cases)
    prompt = build_prompt(error_log, history_text)
    
    # 3. è°ƒç”¨LLM
    response = self.call_llm(prompt)
    
    # 4. è§£æ
    result = parse_json(response)
    return result
```

#### æ”¹é€ å
```python
# å£°æ˜å¼ Chain
self.diagnosis_chain = (
    self.diagnosis_prompt 
    | self.llm 
    | StrOutputParser()
)

# è°ƒç”¨
result = self.diagnosis_chain.invoke({
    "error_log": error_log,
    "retrieved_cases": history_text
})
```

**ä¼˜åŠ¿**ï¼š
- âœ… å£°æ˜å¼ç¼–ç¨‹ï¼Œé€»è¾‘æ¸…æ™°
- âœ… æ˜“äºè°ƒè¯•å’Œæµ‹è¯•
- âœ… æ”¯æŒ Chain ç»„åˆ
- âœ… è‡ªåŠ¨å¤„ç†ä¸­é—´çŠ¶æ€

---

## ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿

### 1. æ ‡å‡†åŒ–
- **ç»Ÿä¸€æ¥å£**ï¼šLLMã€Embeddingsã€VectorStore éƒ½æœ‰æ ‡å‡†æ¥å£
- **ä¸€è‡´æ€§**ï¼šæ‰€æœ‰ç»„ä»¶éµå¾ªç›¸åŒçš„è®¾è®¡æ¨¡å¼
- **äº’æ“ä½œæ€§**ï¼šè½»æ¾åˆ‡æ¢ä¸åŒçš„æ¨¡å‹å’Œå·¥å…·

### 2. å¯æ‰©å±•æ€§

#### åˆ‡æ¢å‘é‡åº“ï¼ˆFAISS â†’ Milvusï¼‰
```python
# åªéœ€ä¿®æ”¹ä¸€è¡Œ
from langchain_community.vectorstores import Milvus

vectorstore = Milvus.from_documents(
    documents, 
    embeddings,
    connection_args={"host": "...", "port": "19530"}
)
```

#### æ·»åŠ  Reranker
```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CohereRerank

compressor = CohereRerank()
retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectorstore.as_retriever()
)
```

#### ä½¿ç”¨ Agent
```python
from langchain.agents import create_openai_functions_agent

tools = [Tool(name="RAGæœç´¢", func=search)]
agent = create_openai_functions_agent(llm, tools, prompt)
```

### 3. é™çº§ä¿æŠ¤

#### è‡ªåŠ¨é™çº§æœºåˆ¶
```python
try:
    # å°è¯•å‘é‡æ£€ç´¢
    results = vectorstore.similarity_search(query)
except Exception:
    # è‡ªåŠ¨é™çº§åˆ°ç®€å•åŒ¹é…
    results = simple_search(query)
```

### 4. ç¤¾åŒºæ”¯æŒ
- ğŸ“š ä¸°å¯Œçš„æ–‡æ¡£å’Œæ•™ç¨‹
- ğŸ› ï¸ æŒç»­æ›´æ–°çš„é›†æˆåº“
- ğŸ’¬ æ´»è·ƒçš„å¼€å‘è€…ç¤¾åŒº
- ğŸ”§ æˆç†Ÿçš„å·¥å…·é“¾

---

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | æ”¹é€ å‰ | æ”¹é€ å |
|------|--------|--------|
| **å‘é‡æ£€ç´¢é€Ÿåº¦** | æ…¢ï¼ˆPython å¾ªç¯ï¼‰ | å¿«ï¼ˆFAISS C++ï¼‰ |
| **å†…å­˜å ç”¨** | é«˜ï¼ˆå¤šæ¬¡å‘é‡åŒ–ï¼‰ | ä½ï¼ˆç¼“å­˜ä¼˜åŒ–ï¼‰ |
| **ä»£ç è¡Œæ•°** | 241è¡Œ | 241è¡Œï¼ˆæ›´æ¸…æ™°ï¼‰ |
| **ç»´æŠ¤æˆæœ¬** | é«˜ | ä½ |
| **æ‰©å±•éš¾åº¦** | éš¾ | æ˜“ |

---

## ğŸš€ åç»­æ‰©å±•å»ºè®®

### 1. æ··åˆæ£€ç´¢ï¼ˆHybrid Searchï¼‰
```python
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

# ç¨ å¯†æ£€ç´¢ï¼ˆå‘é‡ï¼‰
dense_retriever = vectorstore.as_retriever()

# ç¨€ç–æ£€ç´¢ï¼ˆBM25ï¼‰
sparse_retriever = BM25Retriever.from_documents(documents)

# æ··åˆæ£€ç´¢
ensemble_retriever = EnsembleRetriever(
    retrievers=[dense_retriever, sparse_retriever],
    weights=[0.7, 0.3]
)
```

### 2. æµå¼è¾“å‡º
```python
for chunk in self.diagnosis_chain.stream({"error_log": log}):
    print(chunk, end="", flush=True)
```

### 3. å¤šè½®å¯¹è¯
```python
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory()
chain = ConversationChain(llm=llm, memory=memory)
```

### 4. Agent å·¥å…·è°ƒç”¨
```python
from langchain.agents import Tool

tools = [
    Tool(name="æ£€ç´¢æ¡ˆä¾‹", func=rag_search),
    Tool(name="æŸ¥è¯¢ç›‘æ§", func=query_metrics),
    Tool(name="æ‰§è¡Œå‘½ä»¤", func=run_command)
]

agent = create_openai_functions_agent(llm, tools, prompt)
```

---

## ğŸ“ æ€»ç»“

### æ”¹é€ æ”¶ç›Š
- âœ… **ä»£ç è´¨é‡**ï¼šä»æ‰‹å·¥å®ç°å‡çº§åˆ°å·¥ä¸šçº§æ¡†æ¶
- âœ… **å¯ç»´æŠ¤æ€§**ï¼šæ ‡å‡†åŒ–æ¥å£ï¼Œæ˜“äºç†è§£å’Œä¿®æ”¹
- âœ… **å¯æ‰©å±•æ€§**ï¼šè½»æ¾æ·»åŠ æ–°åŠŸèƒ½ï¼ˆRerankerã€Agentç­‰ï¼‰
- âœ… **æ€§èƒ½ä¼˜åŒ–**ï¼šä½¿ç”¨ä¼˜åŒ–çš„åº•å±‚å®ç°ï¼ˆFAISSï¼‰
- âœ… **é™çº§ä¿æŠ¤**ï¼šå¤šå±‚é™çº§ç­–ç•¥ï¼Œæå‡ç³»ç»Ÿç¨³å®šæ€§

### å­¦ä¹ èµ„æº
- [LangChain å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/)
- [LangChain RAG æ•™ç¨‹](https://python.langchain.com/docs/use_cases/question_answering/)
- [FAISS å‘é‡åº“](https://github.com/facebookresearch/faiss)

---

**ç‰ˆæœ¬**: v2.0 (LangChain)  
**æ›´æ–°æ—¶é—´**: 2024-11-19
