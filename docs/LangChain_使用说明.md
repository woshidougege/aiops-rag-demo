# AIOps RAG Demo - LangChain ç‰ˆæœ¬ä½¿ç”¨è¯´æ˜

## ğŸ¯ æ”¹é€ æ¦‚è¿°

æœ¬é¡¹ç›®å·²ä»åŸç”Ÿ API è°ƒç”¨æ”¹é€ ä¸ºåŸºäº **LangChain æ¡†æ¶**çš„å®ç°ï¼Œæå‡äº†ä»£ç çš„å¯ç»´æŠ¤æ€§å’Œæ‰©å±•æ€§ã€‚

## ğŸ”„ ä¸»è¦å˜åŒ–

### 1. æ¡†æ¶å‡çº§
- **åŸå®ç°**ï¼šæ‰‹åŠ¨è°ƒç”¨ APIï¼Œè‡ªå·±å®ç°å‘é‡åŒ–å’Œç›¸ä¼¼åº¦è®¡ç®—
- **æ–°å®ç°**ï¼šä½¿ç”¨ LangChain ç»Ÿä¸€æ¥å£ï¼Œæ ‡å‡†åŒ– RAG æµç¨‹

### 2. æ ¸å¿ƒç»„ä»¶

#### LLM è°ƒç”¨
```python
# åŸæ–¹å¼ï¼šæ‰‹åŠ¨ requests
response = requests.post(api_url, json={...})

# LangChain æ–¹å¼
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(base_url=..., api_key=..., model=...)
response = llm.invoke("your prompt")
```

#### Embeddings
```python
# åŸæ–¹å¼ï¼šæ‰‹åŠ¨è°ƒç”¨ API
response = requests.post(embedding_url, json={...})
vector = response.json()['data'][0]['embedding']

# LangChain æ–¹å¼
from langchain_openai import OpenAIEmbeddings
embeddings = OpenAIEmbeddings(base_url=..., api_key=..., model=...)
vector = embeddings.embed_query("text")
```

#### å‘é‡å­˜å‚¨
```python
# åŸæ–¹å¼ï¼šæ‰‹åŠ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
def cosine_similarity(vec1, vec2):
    # ... æ‰‹åŠ¨å®ç°

# LangChain æ–¹å¼
from langchain_community.vectorstores import FAISS
vectorstore = FAISS.from_documents(documents, embeddings)
results = vectorstore.similarity_search("query", k=3)
```

#### RAG Chain
```python
# åŸæ–¹å¼ï¼šæ‰‹åŠ¨ç»„è£…æµç¨‹
cases = search(query)
prompt = build_prompt(query, cases)
response = call_llm(prompt)

# LangChain æ–¹å¼ï¼šå£°æ˜å¼ Chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

diagnosis_chain = (
    prompt_template 
    | llm 
    | StrOutputParser()
)
result = diagnosis_chain.invoke({"error_log": log})
```

## ğŸ“¦ æ–°å¢ä¾èµ–

```txt
# LangChain æ ¸å¿ƒ
langchain==0.1.0
langchain-community==0.0.10
langchain-core==0.1.10

# LangChain é›†æˆ
langchain-openai==0.0.2
langchain-milvus==0.0.2

# å‘é‡å­˜å‚¨
faiss-cpu==1.7.4

# æ–‡æ¡£å¤„ç†
pypdf==3.17.4
python-docx==1.1.0
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–
```bash
pip install -r requirements.txt
```

### 2. é…ç½®æ–‡ä»¶
`config.py` ä¿æŒä¸å˜ï¼ŒLangChain å…¼å®¹ OpenAI æ ¼å¼çš„ APIï¼š
```python
LLM_CONFIG = {
    "api_base": "http://192.168.20.67:3000/v1",
    "api_key": "sk-xxx",
    "model": "Qwen-32B",
    "temperature": 0.7,
    "max_tokens": 2000
}

EMBEDDING_CONFIG = {
    "api_url": "https://api.siliconflow.cn/v1/embeddings",
    "api_key": "sk-xxx",
    "model": "BAAI/bge-m3",
    "dimension": 1024
}
```

### 3. å¯åŠ¨æœåŠ¡
```bash
cd aiops_demo
python app_simple.py
```

è®¿é—®ï¼š
- Web ç•Œé¢ï¼šhttp://localhost:8888
- API æ–‡æ¡£ï¼šhttp://localhost:8888/docs

## ğŸ” æ ¸å¿ƒåŠŸèƒ½

### 1. å‘é‡æ£€ç´¢
```python
# è‡ªåŠ¨å‘é‡åŒ– + ç›¸ä¼¼åº¦æœç´¢
results = vectorstore.similarity_search_with_score(query, k=3)

# è¿”å›æ ¼å¼
[
    (Document(page_content="...", metadata={...}), score),
    ...
]
```

### 2. æç¤ºæ¨¡æ¿
```python
diagnosis_prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ AIOps è¿ç»´ä¸“å®¶..."),
    ("user", "è¯·åˆ†æä»¥ä¸‹æ•…éšœï¼š{error_log}\nç›¸ä¼¼æ¡ˆä¾‹ï¼š{retrieved_cases}")
])
```

### 3. é“¾å¼è°ƒç”¨
```python
# LCEL (LangChain Expression Language)
chain = prompt | llm | output_parser
result = chain.invoke({"error_log": "..."})
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | åŸå®ç° | LangChain ç‰ˆ |
|------|--------|-------------|
| ä»£ç è¡Œæ•° | 241è¡Œ | 241è¡Œï¼ˆæ›´æ¸…æ™°ï¼‰ |
| å‘é‡æ£€ç´¢ | æ‰‹åŠ¨å®ç° | FAISS ä¼˜åŒ– |
| å¯æ‰©å±•æ€§ | ä½ | é«˜ |
| å¯ç»´æŠ¤æ€§ | ä¸­ | é«˜ |
| è°ƒè¯•éš¾åº¦ | é«˜ | ä½ï¼ˆæ ‡å‡†æ¥å£ï¼‰ |

## ğŸŒŸ ä¼˜åŠ¿

### 1. æ ‡å‡†åŒ–
- ç»Ÿä¸€çš„ LLM è°ƒç”¨æ¥å£
- æ ‡å‡†çš„å‘é‡å­˜å‚¨æ“ä½œ
- è§„èŒƒçš„ Document æ•°æ®ç»“æ„

### 2. å¯æ‰©å±•
- è½»æ¾åˆ‡æ¢å‘é‡åº“ï¼ˆFAISS â†’ Milvus â†’ Pineconeï¼‰
- æ”¯æŒå¤šç§ LLMï¼ˆOpenAIã€Azureã€æœ¬åœ°æ¨¡å‹ï¼‰
- çµæ´»çš„ Chain ç»„åˆ

### 3. ç¤¾åŒºæ”¯æŒ
- ä¸°å¯Œçš„æ–‡æ¡£å’Œç¤ºä¾‹
- æ´»è·ƒçš„ç¤¾åŒº
- æŒç»­æ›´æ–°çš„é›†æˆåº“

### 4. é™çº§æ–¹æ¡ˆ
- å‘é‡å­˜å‚¨å¤±è´¥ â†’ ç®€å•å…³é”®è¯åŒ¹é…
- LLM è°ƒç”¨å¤±è´¥ â†’ è¿”å›æœ€ç›¸ä¼¼æ¡ˆä¾‹
- å¥å£®çš„é”™è¯¯å¤„ç†

## ğŸ”§ æ‰©å±•ç¤ºä¾‹

### ä½¿ç”¨ Milvus æŒä¹…åŒ–å­˜å‚¨
```python
from langchain_community.vectorstores import Milvus

vectorstore = Milvus(
    embedding_function=embeddings,
    connection_args={
        "host": "192.168.1.65",
        "port": "19530"
    },
    collection_name="aiops_knowledge_v1"
)
```

### æ·»åŠ  Reranker
```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CohereRerank

compressor = CohereRerank()
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectorstore.as_retriever()
)
```

### ä½¿ç”¨ Agent
```python
from langchain.agents import create_openai_functions_agent
from langchain.tools import Tool

tools = [
    Tool(
        name="RAGæœç´¢",
        func=rag_engine.search,
        description="æœç´¢å†å²æ•…éšœæ¡ˆä¾‹"
    )
]

agent = create_openai_functions_agent(llm, tools, prompt)
```

## ğŸ“š å‚è€ƒèµ„æ–™

- [LangChain å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/)
- [LangChain RAG æ•™ç¨‹](https://python.langchain.com/docs/use_cases/question_answering/)
- [FAISS å‘é‡åº“](https://github.com/facebookresearch/faiss)
- [Milvus å‘é‡æ•°æ®åº“](https://milvus.io/)

## ğŸ› å¸¸è§é—®é¢˜

### Q1: å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥ï¼Ÿ
**A:** æ£€æŸ¥ `data/knowledge_base.json` æ˜¯å¦å­˜åœ¨ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨é™çº§åˆ°ç®€å•åŒ¹é…ã€‚

### Q2: LLM è°ƒç”¨è¶…æ—¶ï¼Ÿ
**A:** å¢åŠ  `max_tokens` æˆ–è°ƒæ•´ `temperature`ï¼Œç³»ç»Ÿæœ‰é™çº§æ–¹æ¡ˆè¿”å›å†å²æ¡ˆä¾‹ã€‚

### Q3: å¦‚ä½•åˆ‡æ¢åˆ° Milvusï¼Ÿ
**A:** å°† `FAISS.from_documents` æ›¿æ¢ä¸º `Milvus.from_documents`ï¼Œé…ç½®è¿æ¥å‚æ•°å³å¯ã€‚

## ğŸ“ TODO

- [ ] æ·»åŠ æ··åˆæ£€ç´¢ï¼ˆBM25 + å‘é‡ï¼‰
- [ ] é›†æˆ Reranker äºŒæ¬¡æ’åº
- [ ] æ”¯æŒæµå¼è¾“å‡º
- [ ] æ·»åŠ  Agent å·¥å…·è°ƒç”¨
- [ ] å®Œå–„å•å…ƒæµ‹è¯•

---

**æ›´æ–°æ—¶é—´**ï¼š2024-11-19  
**ç‰ˆæœ¬**ï¼šv2.0 (LangChain)
